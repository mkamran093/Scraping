{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"menu_items\":[{\"title\":\"Images\",\"link\":\"https://www.google.com/search?q=marble&sca_esv=a68d2f8b7b299017&gl=us&hl=en&tbm=isch&source=lnms&sa=X&ved=0ahUKEwjlqfW-zf-IAxXRSzABHZcdMD0Q_AUIBigB\",\"position\":1},{\"title\":\"Shopping\",\"link\":\"https://www.google.com/url?url=/search%3Fq%3Dmarble%26sca_esv%3Da68d2f8b7b299017%26gl%3Dus%26hl%3Den%26tbm%3Dshop%26source%3Dlnms%26ved%3D1t:200713%26ictx%3D111&rct=j&q=&esrc=s&opi=89978449&sa=U&ved=0ahUKEwjlqfW-zf-IAxXRSzABHZcdMD0QiaAMCAcoAg&usg=AOvVaw2ZS2aPMnlJsMoxwi-xj7n_\",\"position\":2},{\"title\":\"Videos\",\"link\":\"https://www.google.com/search?q=marble&sca_esv=a68d2f8b7b299017&gl=us&hl=en&tbm=vid&source=lnms&sa=X&ved=0ahUKEwjlqfW-zf-IAxXRSzABHZcdMD0Q_AUICCgD\",\"position\":3},{\"title\":\"Maps\",\"link\":\"https://www.google.com/url?url=https://maps.google.com/maps%3Fq%3Dmarble%26gl%3Dus%26hl%3Den%26um%3D1%26ie%3DUTF-8%26ved%3D1t:200713%26ictx%3D111&rct=j&q=&esrc=s&opi=89978449&sa=U&ved=0ahUKEwjlqfW-zf-IAxXRSzABHZcdMD0QiaAMCAkoBA&usg=AOvVaw11SpNCydyg-j95OSjlGeGk\",\"position\":4},{\"title\":\"News\",\"link\":\"https://www.google.com/search?q=marble&sca_esv=a68d2f8b7b299017&gl=us&hl=en&tbm=nws&source=lnms&sa=X&ved=0ahUKEwjlqfW-zf-IAxXRSzABHZcdMD0Q_AUICigF\",\"position\":5},{\"title\":\"Books\",\"link\":\"https://www.google.com/search?q=marble&sca_esv=a68d2f8b7b299017&gl=us&hl=en&tbm=bks&source=lnms&sa=X&ved=0ahUKEwjlqfW-zf-IAxXRSzABHZcdMD0Q_AUICygG\",\"position\":6}],\"peopleAlsoAskedFor\":[{\"question\":\"What kind of rock is marble?\",\"id\":\"accdef_1\",\"rank\":1,\"displayed_link\":\"https://marble.com › articles › what-type-of-rock-is-marble\",\"link\":\"https://marble.com/articles/what-type-of-rock-is-marble\",\"title\":\"What Type of Rock is Marble?\",\"answer\":\"Marble is a metamorphic rock. Metamorphic rocks are rocks that have undergone a change in composition due to intense heat and pressure. Marble begins as limestone before being subject to the changing process, referred to as metamorphism.\"},{\"question\":\"What does \\\"marble\\\" mean in slang?\",\"id\":\"accdef_3\",\"rank\":2,\"displayed_link\":\"https://www.dictionary.com › browse › marble\",\"link\":\"https://www.dictionary.com/browse/marble\",\"title\":\"MARBLE Definition & Meaning - Dictionary.com\",\"answer\":\"marbles, Slang. normal rational faculties; sanity; wits; common sense: to have all one's marbles; to lose one's marbles.\"},{\"question\":\"Is marble a stone or granite?\",\"id\":\"accdef_5\",\"rank\":3,\"displayed_link\":\"https://www.archcitygranite.com › difference-between-marble-and-granite\",\"link\":\"https://www.archcitygranite.com/difference-between-marble-and-granite\",\"title\":\"Difference Between Marble and Granite\",\"answer\":\"Both granite and marble are naturally found stone materials that are quarried directly from the earth.\"},{\"question\":\"Is marble very expensive?\",\"id\":\"accdef_7\",\"rank\":4,\"displayed_link\":\"https://www.badgergranite.com › marble-countertops-cost-guide\",\"link\":\"https://www.badgergranite.com/marble-countertops-cost-guide/\",\"title\":\"Marble Countertops Cost: A Comprehensive Guide For ...\",\"answer\":\"Marble types, rarities, sizes, and installation complexity all affect pricing, which can range from $50 to $120 per square foot. Options at varying price points are provided by Carrara, Calacatta, and Statuario.\"},{\"question\":\"\",\"rank\":5,\"displayed_link\":\"https://www.badgergranite.com › marble-countertops-cost-guide\",\"link\":\"https://www.badgergranite.com/marble-countertops-cost-guide/\",\"title\":\"Marble Countertops Cost: A Comprehensive Guide For ...\",\"answer\":\"Marble types, rarities, sizes, and installation complexity all affect pricing, which can range from $50 to $120 per square foot. Options at varying price points are provided by Carrara, Calacatta, and Statuario.\"}],\"organic_results\":[{\"title\":\"Marble - Wikipedia\",\"displayed_link\":\"https://en.wikipedia.org › wiki › Marble\",\"snippet\":\"Marble is a metamorphic rock consisting of carbonate minerals (most commonly calcite (CaCO3) or dolomite (CaMg(CO3)2) that have recrystallized under the ...Marble (toy) · Carrara marble · List of types of marble · Tennessee marble\",\"link\":\"https://en.wikipedia.org/wiki/Marble\",\"extended_sitelinks\":[{\"title\":\"Marble (toy)\",\"link\":\"https://en.wikipedia.org/wiki/Marble_(toy)\"},{\"title\":\"Carrara marble\",\"link\":\"https://en.wikipedia.org/wiki/Carrara_marble\"},{\"title\":\"List of types of marble\",\"link\":\"https://en.wikipedia.org/wiki/List_of_types_of_marble\"},{\"title\":\"Tennessee marble\",\"link\":\"https://en.wikipedia.org/wiki/Tennessee_marble\"}],\"rank\":1},{\"title\":\"Marble Brewery - Unabashedly Bold Beer\",\"displayed_link\":\"https://marblebrewery.com\",\"snippet\":\"Punctiliously Brewed In Abq NM USA. Founded in 2008 in the heart of Downtown Albuquerque, Marble Brewery is devoted to brewing premium craft beer that satisfies ...\",\"link\":\"https://marblebrewery.com/\",\"rank\":2},{\"title\":\"Marble Countertops & Slabs - MSI Surfaces\",\"displayed_link\":\"https://www.msisurfaces.com › marble-countertops\",\"snippet\":\"MSI Surfaces is the leading U.S. importer and wholesale supplier of Marble countertops and slabs with a variety of marble colors to choose from.Portinari Marble · Calacatta Gold Marble · White Carrara Marble · Fantasy River\",\"link\":\"https://www.msisurfaces.com/marble-countertops/\",\"extended_sitelinks\":[{\"title\":\"Portinari Marble\",\"link\":\"https://www.msisurfaces.com/marble/portinari-marble/\"},{\"title\":\"Calacatta Gold Marble\",\"link\":\"https://www.msisurfaces.com/marble/calacatta-marble/\"},{\"title\":\"White Carrara Marble\",\"link\":\"https://www.msisurfaces.com/marble/carrara-white/\"},{\"title\":\"Fantasy River\",\"link\":\"https://www.msisurfaces.com/marble/fantasy-river/\"}],\"rank\":3},{\"title\":\"Marble Law | Your Family & immigration law firm\",\"displayed_link\":\"https://www.marble.co\",\"snippet\":\"We are Marble - a nationwide law firm focusing on family & immigration law. Click to discover our experienced team of lawyers, how we price, and more.\",\"link\":\"https://www.marble.co/\",\"rank\":4},{\"title\":\"Marble\",\"displayed_link\":\"https://\",\"snippet\":\"\",\"link\":null,\"extended_sitelinks\":[{\"title\":\"Wikipedia\",\"link\":\"https://en.wikipedia.org/wiki/Marble\"}],\"rank\":5},{\"title\":\"San Luis MarbleFresno Marble & GraniteFrancini Inc. Marble & Granite - Boise\",\"displayed_link\":\"https://\",\"snippet\":\"\",\"link\":\"https://maps.google.com/maps?gl=us&hl=en&um=1&ie=UTF-8&fb=1&sa=X&sll=39.418025,-118.487192&sspn=13.1519281,22.7389799&q=marble&ved=1t:200245&ictx=111\",\"rank\":6},{\"title\":\"Marble Insurance | Save time and money on insurance\",\"displayed_link\":\"https://www.marblepay.com\",\"snippet\":\"\\\"Marble is a unique, valuable, relevant, and rewarding app that you should definitely check out. Not only are all your insurance policies organized in one place ...\",\"link\":\"https://www.marblepay.com/\",\"rank\":7},{\"title\":\"Marble: Metamorphic Rock: Pictures, Definition, Properties\",\"displayed_link\":\"https://geology.com › rocks › marble\",\"snippet\":\"Marble is a non-foliated metamorphic rock that forms through the metamorphism of limestone. It has a greater number of potential uses than almost any other ...\",\"link\":\"https://geology.com/rocks/marble.shtml\",\"rank\":8},{\"title\":\"Marble - Geology - rocks and minerals - The University of Auckland\",\"displayed_link\":\"https://rocksminerals.flexiblelearning.auckland.ac.nz › rocks › marble\",\"snippet\":\"Marble is a metamorphic rock formed when limestone is exposed to high temperatures and pressures. Marble forms under such conditions because the calcite forming ...\",\"link\":\"https://rocksminerals.flexiblelearning.auckland.ac.nz/rocks/marble.html\",\"rank\":9},{\"title\":\"Irish Marble gifts, Unique Marble\",\"displayed_link\":\"https://connemaramarble.com › product-category › marble-gifts\",\"snippet\":\"Handcrafted in Ireland with 900 million year old Connemara Marble, our Irish marble gifts are unique to you. Sign up to our newsletter to receive 15% off.\",\"link\":\"https://connemaramarble.com/product-category/marble-gifts/\",\"rank\":10},{\"title\":\"Marble Technologies | Meat Processing Automation\",\"displayed_link\":\"https://www.seemarble.com\",\"snippet\":\"End-to-End Automation. Marble builds AI and food-grade hardware to reduce labor, improve product quality, and reduce waste in meat processing.Marble Pack-Off · Marble Box Verification · Careers · About Us\",\"link\":\"https://www.seemarble.com/\",\"extended_sitelinks\":[{\"title\":\"Marble Pack-Off\",\"link\":\"https://www.seemarble.com/pack-off\"},{\"title\":\"Marble Box Verification\",\"link\":\"https://www.seemarble.com/box-verification\"},{\"title\":\"Careers\",\"link\":\"https://www.seemarble.com/careers\"},{\"title\":\"About Us\",\"link\":\"https://www.seemarble.com/about\"}],\"rank\":11}],\"relatedSearches\":[{\"query\":\"Marble Ball\",\"link\":\"https://google.com/search?gl=us&q=Marble+Ball\"},{\"query\":\"Marble rock\",\"link\":\"https://google.com/search?gl=us&q=Marble+rock\"},{\"query\":\"Marble slang\",\"link\":\"https://google.com/search?gl=us&q=Marble+slang\"},{\"query\":\"Marble definition\",\"link\":\"https://google.com/search?gl=us&q=Marble+definition\"},{\"query\":\"Marble stone\",\"link\":\"https://google.com/search?gl=us&q=Marble+stone\"},{\"query\":\"Marble Toy\",\"link\":\"https://google.com/search?gl=us&q=Marble+Toy\"},{\"query\":\"Marble price\",\"link\":\"https://google.com/search?gl=us&q=Marble+price\"},{\"query\":\"What is marble used for\",\"link\":\"https://google.com/search?gl=us&q=What+is+marble+used+for\"}],\"pagination\":{\"page_no\":{}}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "payload = {'api_key': '63655678cd36805b2cb76220', 'query':'marble' , 'country':'us'}\n",
    "resp = requests.get('https://api.scrapingdog.com/google', params=payload)\n",
    "print (resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results found in the JSON response.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "  \n",
    "api_key = \"63655678cd36805b2cb76220\"\n",
    "url = \"https://api.scrapingdog.com/google\"\n",
    "  \n",
    "params = {\n",
    "    \"api_key\": api_key,\n",
    "    \"query\": \"marble & granite\",\n",
    "    \"results\": 1000,\n",
    "    \"country\": \"us\",\n",
    "    \"page\": 0\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    \n",
    "    results = data.get('results', []) \n",
    "    \n",
    "    if results:\n",
    "        headers = results[0].keys()\n",
    "        with open('output.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(results)\n",
    "        \n",
    "        print(\"Data has been written to output.csv successfully.\")\n",
    "    else:\n",
    "        print(\"No results found in the JSON response.\")\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/satch/Desktop/marble/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/satch/Desktop/marble/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m dataframes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     10\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/satch/Desktop/marble/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "folder_path = 'C:/Users/satch/Desktop/marble/'\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, engine='python', sep=',', on_bad_lines='warn') \n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "combined_df = pd.concat(dataframes, axis=0, ignore_index=True, sort=False)\n",
    "combined_df.to_csv('marble_tracker.csv', index=False)\n",
    "\n",
    "print(\"All CSV files combined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'marble_tracker.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the new CSV\u001b[39;00m\n\u001b[0;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarble_tracker.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#df = df[['name', 'reviews', 'categories', 'rating', 'address']]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\NeXbit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NeXbit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\NeXbit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NeXbit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\NeXbit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'marble_tracker.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new CSV\n",
    "file_path = 'marble_tracker.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "#df = df[['name', 'reviews', 'categories', 'rating', 'address']]\n",
    "df.head()\n",
    "\n",
    "# # Drop duplicate rows based on the 'name' column, keeping the first occurrence\n",
    "df_cleaned = df.drop_duplicates(subset=['name'], keep='first')\n",
    "\n",
    "# # Save the cleaned DataFrame to a new CSV file (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74.83333333333333, 2806.25)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Function to calculate hours between two time periods\n",
    "def calculate_hours(start, end):\n",
    "    fmt = '%H:%M'\n",
    "    start = datetime.strptime(start, fmt)\n",
    "    end = datetime.strptime(end, fmt)\n",
    "    return (end - start).seconds / 3600\n",
    "\n",
    "# Hours for each day\n",
    "days = {\n",
    "    \"18/09/2024\": [(\"8:30\", \"12:00\"), (\"13:00\", \"19:30\")],\n",
    "    \"19/09/2024\": [(\"8:30\", \"12:00\"), (\"13:00\", \"18:30\")],\n",
    "    \"20/09/2024\": [(\"9:00\", \"12:30\"), (\"14:00\", \"18:30\")],\n",
    "    \"23/09/2024\": [(\"8:30\", \"12:30\"), (\"13:30\", \"17:30\")],\n",
    "    \"24/09/2024\": [(\"8:30\", \"13:30\"), (\"14:30\", \"18:00\")],\n",
    "    \"25/09/2024\": [(\"8:30\", \"12:00\"), (\"13:00\", \"17:00\")],\n",
    "    \"26/09/2024\": [(\"8:30\", \"12:00\"), (\"13:00\", \"17:00\")],\n",
    "    \"27/09/2024\": [(\"8:30\", \"11:45\"), (\"12:45\", \"17:30\")],\n",
    "    \"30/09/2024\": [(\"8:30\", \"12:20\"), (\"12:50\", \"17:20\")]\n",
    "}\n",
    "\n",
    "# Calculate total hours\n",
    "total_hours = sum(\n",
    "    calculate_hours(start, end) for day in days for start, end in days[day]\n",
    ")\n",
    "\n",
    "# Multiply total hours by 37.5\n",
    "total_hours_multiplied = total_hours * 37.5\n",
    "total_hours, total_hours_multiplied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Test URL to scrape roofing companies from Yelp (example URL)\n",
    "url = 'https://www.yelp.com/search?find_desc=Roofing&find_loc=United+States'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Lists to store the extracted data\n",
    "company_names = []\n",
    "phone_numbers = []\n",
    "websites = []\n",
    "\n",
    "# Find business listings\n",
    "for business in soup.find_all('div', class_='container__09f24__sxa9-'):\n",
    "    name = business.find('a', class_='css-1422juy').text if business.find('a', class_='css-1422juy') else 'N/A'\n",
    "    phone = business.find('p', class_='css-8jxw1i').text if business.find('p', class_='css-8jxw1i') else 'N/A'\n",
    "    website = business.find('a', class_='css-1um3nx')[\"href\"] if business.find('a', class_='css-1um3nx') else 'N/A'\n",
    "    \n",
    "    company_names.append(name)\n",
    "    phone_numbers.append(phone)\n",
    "    websites.append(website)\n",
    "\n",
    "# Create a DataFrame to store the data\n",
    "df = pd.DataFrame({\n",
    "    'Company Name': company_names,\n",
    "    'Phone Number': phone_numbers,\n",
    "    'Website': websites\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('roofing_companies_test.csv', index=False)\n",
    "\n",
    "print(\"CSV file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total USD: $2193.75\n",
      "Total BRL: R$11136.00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import requests  # If using an external API for the exchange rate\n",
    "\n",
    "def calculate_hours(start, end):\n",
    "    fmt = '%H:%M'\n",
    "    start = datetime.strptime(start, fmt)\n",
    "    end = datetime.strptime(end, fmt)\n",
    "    return (end - start).seconds / 3600\n",
    "\n",
    "def total_hours_worked(work_days):\n",
    "    total_hours = 0\n",
    "    for date, intervals in work_days.items():\n",
    "        for start, end in intervals:\n",
    "            total_hours += calculate_hours(start, end)\n",
    "    return total_hours\n",
    "\n",
    "def get_hourly_rate():\n",
    "    # Assuming a constant hourly rate in USD\n",
    "    return 37.5\n",
    "\n",
    "def get_exchange_rate(date):\n",
    "    # Placeholder for exchange rate API call\n",
    "    # Example of calling an external API for the exchange rate\n",
    "    # response = requests.get(f'https://api.exchangerate.com/{date}')\n",
    "    # return response.json()['USD_to_BRL']\n",
    "    \n",
    "    # For now, we return a mock exchange rate (replace with actual rate-fetching code)\n",
    "    exchange_rates = {\n",
    "        \"18/09/2024\": 5.0,\n",
    "        \"19/09/2024\": 5.1,\n",
    "        \"20/09/2024\": 5.05,\n",
    "        \"23/09/2024\": 5.08,\n",
    "        \"24/09/2024\": 5.12,\n",
    "        \"25/09/2024\": 5.09,\n",
    "        \"26/09/2024\": 5.11,\n",
    "    }\n",
    "    return exchange_rates.get(date, 5.0)  # Default exchange rate if date not found\n",
    "\n",
    "def calculate_total_in_usd_and_brl(work_days):\n",
    "    total_usd = 0\n",
    "    total_brl = 0\n",
    "    hourly_rate = get_hourly_rate()\n",
    "    \n",
    "    for date, intervals in work_days.items():\n",
    "        hours = sum(calculate_hours(start, end) for start, end in intervals)\n",
    "        daily_total_usd = hours * hourly_rate\n",
    "        exchange_rate = get_exchange_rate(date)\n",
    "        daily_total_brl = daily_total_usd * exchange_rate\n",
    "        \n",
    "        total_usd += daily_total_usd\n",
    "        total_brl += daily_total_brl\n",
    "        \n",
    "    return total_usd, total_brl\n",
    "\n",
    "# Example usage:\n",
    "work_days = {\n",
    "    \"18/09/2024\": [(\"8:30\", \"12:00\"), (\"13:00\", \"19:30\")],\n",
    "    \"19/09/2024\": [(\"8:30\", \"12:00\"), (\"13:00\", \"18:30\")],\n",
    "    \"20/09/2024\": [(\"9:00\", \"12:30\"), (\"14:00\", \"18:30\")],\n",
    "    \"23/09/2024\": [(\"8:30\", \"12:30\"), (\"13:30\", \"17:30\")],\n",
    "    \"24/09/2024\": [(\"8:30\", \"13:30\"), (\"14:30\", \"18:00\")],\n",
    "    \"25/09/2024\": [(\"8:30\", \"12:00\"), (\"13:00\", \"17:00\")],\n",
    "    \"26/09/2024\": [(\"8:30\", \"12:00\"), (\"13:00\", \"17:00\")],\n",
    "}\n",
    "\n",
    "total_usd, total_brl = calculate_total_in_usd_and_brl(work_days)\n",
    "print(f\"Total USD: ${total_usd:.2f}\")\n",
    "print(f\"Total BRL: R${total_brl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'marble_boston_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load company names from the CSV\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmarble_boston_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate of Organization\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOwner\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\NeXbit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NeXbit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\NeXbit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\NeXbit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\NeXbit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'marble_boston_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Load company names from the CSV\n",
    "df = pd.read_csv('marble_boston_data.csv')\n",
    "df['Date of Organization'] = ''\n",
    "df['Owner'] = ''\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    company_name = row['name'].strip()  # Strip any extra spaces from the company name\n",
    "\n",
    "    try:\n",
    "        # Step 1: Open the corporation search page\n",
    "        driver.get('https://corp.sec.state.ma.us/corpweb/CorpSearch/CorpSearch.aspx')\n",
    "\n",
    "        # Step 2: Wait for the search box to load and enter the company name\n",
    "        search_box = WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.ID, \"MainContent_txtEntityName\"))\n",
    "        )\n",
    "        search_box.clear()\n",
    "        search_box.send_keys(company_name)\n",
    "\n",
    "        # Step 3: Click the search button\n",
    "        search_button = driver.find_element(By.ID, 'MainContent_btnSearch')\n",
    "        search_button.click()\n",
    "\n",
    "        # Step 4: Wait for the results table to load and locate the first company link\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.ID, \"MainContent_grdSearchResultsEntity\"))\n",
    "        )\n",
    "\n",
    "        # Step 5: Find the link for the company using partial match, case-insensitive\n",
    "        company_link = driver.find_element(By.XPATH, f\"//a[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{company_name.lower()}')]\")\n",
    "\n",
    "        # Step 6: Extract the href attribute from the link\n",
    "        company_href = company_link.get_attribute(\"href\")\n",
    "        print(f\"Copying link: {company_href}\")\n",
    "\n",
    "        # Step 7: Open the extracted URL by navigating directly\n",
    "        driver.get(company_href)  # Opening the second page with the copied link\n",
    "\n",
    "        # Step 8: Wait for the company summary page to load\n",
    "        WebDriverWait(driver, 30).until(\n",
    "            EC.url_contains(\"CorpSummary.aspx\")\n",
    "        )\n",
    "\n",
    "        # Step 9: Scrape the required details from the company summary page\n",
    "        date_of_organization = WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//td[text()='Date of Organization:']/following-sibling::td\"))\n",
    "        ).text.strip()\n",
    "\n",
    "        owner = driver.find_element(By.XPATH, \"//td[text()='Address:']/following-sibling::td\").text.strip()\n",
    "\n",
    "        # Step 10: Populate the DataFrame with the extracted data\n",
    "        df.at[index, 'Date of Organization'] = date_of_organization\n",
    "        df.at[index, 'Owner'] = owner\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {company_name}: {e}\")\n",
    "\n",
    "    time.sleep(3)  # Delay to avoid overwhelming the server\n",
    "\n",
    "# Step 11: Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Step 12: Save the updated DataFrame back to a CSV\n",
    "df.to_csv('updated_marble_boston_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened website: https://marbleoficial.com.br/\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Function to search Google and open the first result\n",
    "def search_and_open(query):\n",
    "    # Setup Selenium WebDriver\n",
    "    options = Options()\n",
    "    options.headless = False  # Set to True if you want to run headlessly\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    try:\n",
    "        # Navigate to Google\n",
    "        driver.get(\"https://www.google.com/\")\n",
    "        \n",
    "        # Find the search box, enter the query, and submit\n",
    "        search_box = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"q\"))\n",
    "        )\n",
    "        search_box.send_keys(query)\n",
    "        search_box.submit()\n",
    "\n",
    "        # Wait for the results to load and click the first result\n",
    "        first_result = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, 'h3'))\n",
    "        )\n",
    "        first_result.click()\n",
    "        \n",
    "        # Wait a moment to ensure the page loads\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Print the current URL of the opened website\n",
    "        print(f\"Opened website: {driver.current_url}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Close the driver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter the word you want to search for: \")\n",
    "    search_and_open(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Load CSV file\n",
    "input_csv = 'marble_boston.csv'\n",
    "output_csv = 'marble_boston_personal.csv'\n",
    "\n",
    "# Read the input CSV\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Prepare a list to store scraped data\n",
    "data = []\n",
    "\n",
    "# Load existing progress if the output CSV already exists\n",
    "if os.path.exists(output_csv):\n",
    "    existing_data = pd.read_csv(output_csv)\n",
    "    processed_links = existing_data['Website'].tolist()\n",
    "else:\n",
    "    processed_links = []\n",
    "\n",
    "# Setup Selenium WebDriver\n",
    "options = Options()\n",
    "options.headless = False  # Set to True if you want to run headlessly\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Function to search for the official website on Google\n",
    "def search_google(name, address):\n",
    "    search_query = f\"{name} {address} official site\"\n",
    "    driver.get(\"https://www.google.com/\")\n",
    "    try:\n",
    "        # Find the search box, enter the query, and submit\n",
    "        search_box = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"q\"))\n",
    "        )\n",
    "        search_box.send_keys(search_query)\n",
    "        search_box.submit()\n",
    "\n",
    "        # Wait for the results to load and click the first result\n",
    "        first_result = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, 'h3'))\n",
    "        )\n",
    "        first_result.click()\n",
    "        \n",
    "        # Get the current URL after clicking\n",
    "        official_website = driver.current_url\n",
    "        print(f\"Clicked on official website: {official_website}\")\n",
    "        return official_website\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching Google for {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to scrape contact info from the official website\n",
    "def scrape_contact_info():\n",
    "    try:\n",
    "        # Wait for the page to load and get the telephone number\n",
    "        telephone = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, 'tel:')]\"))\n",
    "        ).text.strip()\n",
    "        return telephone\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping contact info: {e}\")\n",
    "        return None\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    name = row['name']  # Use your actual column name for the name\n",
    "    address = row['address']  # Use your actual column name for the address\n",
    "    \n",
    "    # Skip if the website has already been processed\n",
    "    if name in processed_links:\n",
    "        continue\n",
    "\n",
    "    # Search for the official website\n",
    "    official_website = search_google(name, address)\n",
    "\n",
    "    if official_website:\n",
    "        # Scrape contact info from the official website\n",
    "        telephone = scrape_contact_info()\n",
    "\n",
    "        # Store the data if successfully scraped\n",
    "        if telephone:\n",
    "            data.append({\n",
    "                'Name': name,\n",
    "                'Address': address,\n",
    "                'Website': official_website,\n",
    "                'Telephone': telephone\n",
    "            })\n",
    "\n",
    "            # Save progress after each successful scrape\n",
    "            output_df = pd.DataFrame(data)\n",
    "            output_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    # Longer sleep interval to avoid detection\n",
    "    time.sleep(random.uniform(5, 10))\n",
    "\n",
    "# Final save of any remaining data\n",
    "if data:\n",
    "    output_df = pd.DataFrame(data)\n",
    "    output_df.to_csv(output_csv, index=False)\n",
    "\n",
    "driver.quit()\n",
    "print(f\"Scraping completed! Data saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid proxy server credentials supplied\n",
      "Invalid proxy server credentials supplied\n",
      "Invalid proxy server credentials supplied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No consent required.\n",
      "Screenshot saved to /path/to/your/destination/screenshot.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid proxy server credentials supplied\n",
      "Invalid proxy server credentials supplied\n",
      "Invalid proxy server credentials supplied\n",
      "Invalid proxy server credentials supplied\n",
      "Invalid proxy server credentials supplied\n",
      "Invalid proxy server credentials supplied\n",
      "Invalid proxy server credentials supplied\n",
      "Invalid proxy server credentials supplied\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id='QA0Szd']/div/div/div[1]/div[2]/div\"}\n  (Session info: chrome=129.0.6668.71); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x005A6AB3+25587]\n\t(No symbol) [0x00539C54]\n\t(No symbol) [0x00432113]\n\t(No symbol) [0x00476F62]\n\t(No symbol) [0x004771AB]\n\t(No symbol) [0x004B7852]\n\t(No symbol) [0x0049ABE4]\n\t(No symbol) [0x004B5370]\n\t(No symbol) [0x0049A936]\n\t(No symbol) [0x0046BA73]\n\t(No symbol) [0x0046C4CD]\n\tGetHandleVerifier [0x00884C63+3032483]\n\tGetHandleVerifier [0x008D6B99+3368153]\n\tGetHandleVerifier [0x00638F62+624802]\n\tGetHandleVerifier [0x006407DC+655644]\n\t(No symbol) [0x0054260D]\n\t(No symbol) [0x0053F6D8]\n\t(No symbol) [0x0053F875]\n\t(No symbol) [0x00531CA6]\n\tBaseThreadInitThunk [0x768B7BA9+25]\n\tRtlInitializeExceptionChain [0x7728C0CB+107]\n\tRtlClearBits [0x7728C04F+191]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(pause_time)\n\u001b[0;32m     68\u001b[0m panel_xpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//*[@id=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQA0Szd\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]/div/div/div[1]/div[2]/div\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 69\u001b[0m \u001b[43mscroll_panel_with_page_down\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpanel_xpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpause_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Get the page HTML source\u001b[39;00m\n\u001b[0;32m     72\u001b[0m page_source \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "Cell \u001b[1;32mIn[3], line 55\u001b[0m, in \u001b[0;36mscroll_panel_with_page_down\u001b[1;34m(driver, panel_xpath, presses, pause_time)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03mScrolls within a specific panel by simulating Page Down key presses.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m:param pause_time: Time to pause between key presses, in seconds.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Find the panel element\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m panel_element \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpanel_xpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Ensure the panel is in focus by clicking on it\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Note: Some elements may not need or allow clicking to focus. Adjust as needed.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m actions \u001b[38;5;241m=\u001b[39m ActionChains(driver)\n",
      "File \u001b[1;32mc:\\Users\\satch\\Desktop\\codAI\\trf\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:748\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    745\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    746\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\satch\\Desktop\\codAI\\trf\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\satch\\Desktop\\codAI\\trf\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id='QA0Szd']/div/div/div[1]/div[2]/div\"}\n  (Session info: chrome=129.0.6668.71); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x005A6AB3+25587]\n\t(No symbol) [0x00539C54]\n\t(No symbol) [0x00432113]\n\t(No symbol) [0x00476F62]\n\t(No symbol) [0x004771AB]\n\t(No symbol) [0x004B7852]\n\t(No symbol) [0x0049ABE4]\n\t(No symbol) [0x004B5370]\n\t(No symbol) [0x0049A936]\n\t(No symbol) [0x0046BA73]\n\t(No symbol) [0x0046C4CD]\n\tGetHandleVerifier [0x00884C63+3032483]\n\tGetHandleVerifier [0x008D6B99+3368153]\n\tGetHandleVerifier [0x00638F62+624802]\n\tGetHandleVerifier [0x006407DC+655644]\n\t(No symbol) [0x0054260D]\n\t(No symbol) [0x0053F6D8]\n\t(No symbol) [0x0053F875]\n\t(No symbol) [0x00531CA6]\n\tBaseThreadInitThunk [0x768B7BA9+25]\n\tRtlInitializeExceptionChain [0x7728C0CB+107]\n\tRtlClearBits [0x7728C04F+191]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid proxy server credentials supplied\n",
      "Invalid proxy server credentials supplied\n",
      "Invalid proxy server credentials supplied\n",
      "Invalid proxy server credentials supplied\n"
     ]
    }
   ],
   "source": [
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Selenium Wire configuration to use a proxy\n",
    "proxy_username = 'username'\n",
    "proxy_password = 'password'\n",
    "seleniumwire_options = {\n",
    "    'proxy': {\n",
    "        'http': f'http://{proxy_username}:{proxy_password}@city.smartproxy.com:21250',\n",
    "        'verify_ssl': False,\n",
    "    },\n",
    "}\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, seleniumwire_options=seleniumwire_options)\n",
    "\n",
    "# URL of the web page\n",
    "url = \"https://www.google.com/maps/search/falafel+in+london/\"\n",
    "\n",
    "# Open the web page\n",
    "driver.get(url)\n",
    "\n",
    "try: \n",
    "    button = driver.find_element(By.XPATH,\"//button[@class='VfPpkd-LgbsSe VfPpkd-LgbsSe-OWXEXe-k8QpJ VfPpkd-LgbsSe-OWXEXe-dgl2Hf nCP5yc AjY5Oe DuMIQc LQeN7 XWZjwc']\") \n",
    "    button.click()\n",
    "    print(\"Clicked consent to cookies.\") \n",
    "except: \n",
    "    print(\"No consent required.\")\n",
    "\n",
    "# Set an implicit wait time to wait for JavaScript to render\n",
    "driver.implicitly_wait(30)  # Wait for max 30 seconds\n",
    "\n",
    "# Take a screenshot after the content you want is loaded\n",
    "screenshot_path = '/path/to/your/destination/screenshot.png'\n",
    "driver.save_screenshot(screenshot_path)\n",
    "print(f\"Screenshot saved to {screenshot_path}\")\n",
    "\n",
    "def scroll_panel_with_page_down(driver, panel_xpath, presses, pause_time):\n",
    "    \"\"\"\n",
    "    Scrolls within a specific panel by simulating Page Down key presses.\n",
    "\n",
    "    :param driver: The Selenium WebDriver instance.\n",
    "    :param panel_xpath: The XPath to the panel element.\n",
    "    :param presses: The number of times to press the Page Down key.\n",
    "    :param pause_time: Time to pause between key presses, in seconds.\n",
    "    \"\"\"\n",
    "    # Find the panel element\n",
    "    panel_element = driver.find_element(By.XPATH, panel_xpath)\n",
    "    \n",
    "    # Ensure the panel is in focus by clicking on it\n",
    "    # Note: Some elements may not need or allow clicking to focus. Adjust as needed.\n",
    "    actions = ActionChains(driver)\n",
    "    actions.move_to_element(panel_element).click().perform()\n",
    "\n",
    "    # Send the Page Down key to the panel element\n",
    "    for _ in range(presses):\n",
    "        actions = ActionChains(driver)\n",
    "        actions.send_keys(Keys.PAGE_DOWN).perform()\n",
    "        time.sleep(pause_time)\n",
    "\n",
    "panel_xpath = \"//*[@id='QA0Szd']/div/div/div[1]/div[2]/div\"\n",
    "scroll_panel_with_page_down(driver, panel_xpath, presses=5, pause_time=1)\n",
    "\n",
    "# Get the page HTML source\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Parse the HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "# Find all elements using its class\n",
    "titles = soup.find_all(class_=\"hfpxzc\")\n",
    "ratings = soup.find_all(class_='MW4etd')\n",
    "reviews = soup.find_all(class_='UY7F9')\n",
    "services = soup.find_all(class_='Ahnjwc')\n",
    "\n",
    "# Print the number of places found\n",
    "elements_count = len(titles)\n",
    "print(f\"Number of places found: {elements_count}\")\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file_path = '/path/to/your/destination/places.csv'\n",
    "\n",
    "# Open a CSV file in write mode\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    # Create a CSV writer object\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    # Write the header row (optional, adjust according to your data)\n",
    "    csv_writer.writerow(['Place', 'Rating', 'Reviews', 'Service options'])\n",
    "    \n",
    "    # Write the extracted data\n",
    "    for i, title in enumerate(titles):\n",
    "        title = title.get('aria-label')\n",
    "        rating = (ratings[i].text + \"/5\") if i < len(ratings) else 'N/A' # Ensure we have a rating and reviews for each title, defaulting to 'N/A' if not found\n",
    "        review_count = reviews[i].text if i < len(reviews) else 'N/A'\n",
    "        service = services[i].text if i < len(services) else 'N/A'\n",
    "\n",
    "        # Write a row to the CSV file\n",
    "        if title:\n",
    "            csv_writer.writerow([title, rating, review_count, service])\n",
    "\n",
    "print(f\"Data has been saved to '{csv_file_path}'\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting blinker==1.5\n",
      "  Downloading blinker-1.5-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
      "Installing collected packages: blinker\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.8.2\n",
      "    Uninstalling blinker-1.8.2:\n",
      "      Successfully uninstalled blinker-1.8.2\n",
      "Successfully installed blinker-1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flask 3.0.3 requires blinker>=1.6.2, but you have blinker 1.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install blinker==1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in DataFrame: ['name', 'reviews', 'categories', 'rating', 'address', 'map_link', 'website', 'telephone']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "input_csv = 'marble_boston_personal.csv'\n",
    "output_csv = 'marble_boston_data.csv'\n",
    "df = pd.read_csv(input_csv)\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "print(\"Columns in DataFrame:\", df.columns.tolist())  # Debugging line\n",
    "\n",
    "def clean_and_format_phone(phone):\n",
    "    if isinstance(phone, str):\n",
    "        digits = re.sub(r'\\D', '', phone)\n",
    "        return f\"({digits[:3]}) {digits[3:6]}-{digits[6:]}\" if len(digits) == 10 else ''\n",
    "    return ''\n",
    "\n",
    "if 'telephone' in df.columns:\n",
    "    df['telephone'] = df['telephone'].apply(clean_and_format_phone)\n",
    "else:\n",
    "    print(\"Column 'telephone' not found in the DataFrame.\")\n",
    "\n",
    "df['categories'] = 'granite & marble'\n",
    "df['address'] = df['address'].str.replace('Estados Unidos', 'United States')\n",
    "df['website'] = df['website'].replace('https://www.google.com', '')\n",
    "df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No company websites found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Setup Selenium WebDriver with ChromeDriverManager\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.headless = False  # Keep headless mode off to see the browser (set to True for headless)\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "    # Initialize WebDriver using ChromeDriverManager\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    return driver\n",
    "\n",
    "# Function to check if the URL is a valid company website (ends with .com, .org, etc.)\n",
    "def is_valid_company_url(url):\n",
    "    invalid_keywords = [\"google.com\", \"facebook.com\", \"yelp.com\", \"linkedin.com\", \"twitter.com\", \"maps.google.com\"]\n",
    "    \n",
    "    if any(keyword in url for keyword in invalid_keywords):\n",
    "        return False\n",
    "    if url.endswith(\".com\") or url.endswith(\".org\") or url.endswith(\".net\") or url.endswith(\".biz\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to scrape companies using Google search for a specific niche and location\n",
    "def google_search_companies(driver, niche, location):\n",
    "    search_query = f\"{niche} companies in {location}\"\n",
    "    \n",
    "    # Navigate to Google\n",
    "    driver.get(\"https://www.google.com/\")\n",
    "    \n",
    "    # Wait for the Google search bar to load and enter the query\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.NAME, \"q\")))\n",
    "    \n",
    "    search_input = driver.find_element(By.NAME, \"q\")\n",
    "    search_input.send_keys(search_query)\n",
    "    search_input.send_keys(Keys.RETURN)\n",
    "    \n",
    "    # Wait for the search results to load\n",
    "    WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div.g\")))\n",
    "    \n",
    "    company_list = []\n",
    "    \n",
    "    try:\n",
    "        result_elements = driver.find_elements(By.CSS_SELECTOR, \"div.g\")  # Each search result block\n",
    "        \n",
    "        for result in result_elements:\n",
    "            try:\n",
    "                url = result.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "                \n",
    "                # Check if the URL is a valid company website\n",
    "                if is_valid_company_url(url):\n",
    "                    company_list.append({\n",
    "                        'URL': url\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting URL: {e}\")\n",
    "                continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {e}\")\n",
    "    \n",
    "    return company_list\n",
    "\n",
    "# Save the data to CSV\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Main function to run the scraper\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    niche = \"technology\"  # Example niche\n",
    "    location = \"Boston\"   # Example location\n",
    "    \n",
    "    # Perform the Google search\n",
    "    company_data = google_search_companies(driver, niche, location)\n",
    "    \n",
    "    if company_data:\n",
    "        # Save to CSV\n",
    "        save_to_csv(company_data, f\"company_websites_{niche}_{location}.csv\")\n",
    "    else:\n",
    "        print(\"No company websites found.\")\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satch\\AppData\\Local\\Temp\\ipykernel_5780\\2636668411.py:57: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  owner = soup.find(text=re.compile(r'Owner|CEO|Founder', re.I))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Chrome options for headless browsing\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Comment this line for debugging\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "def search_google(company_name):\n",
    "    driver.get(\"https://www.google.com\")\n",
    "    search_box = driver.find_element(By.NAME, \"q\")\n",
    "    search_box.send_keys(company_name)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    try:\n",
    "        # Wait until the search results are loaded\n",
    "        website_link = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, 'http')]\"))\n",
    "        )\n",
    "        return website_link.get_attribute(\"href\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding website link: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_contact_info(url):\n",
    "    contact_info = {'phone': None, 'email': None, 'owner': None}\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the page to load and content to be available\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Extract phone number using regex\n",
    "        phone_match = re.search(r'(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', soup.text)\n",
    "        contact_info['phone'] = phone_match.group(0) if phone_match else None\n",
    "\n",
    "        # Extract email using regex\n",
    "        email_match = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', soup.text)\n",
    "        contact_info['email'] = email_match.group(0) if email_match else None\n",
    "\n",
    "        # Extract owner/CEO name by looking for keywords\n",
    "        owner = soup.find(text=re.compile(r'Owner|CEO|Founder', re.I))\n",
    "        contact_info['owner'] = owner.strip() if owner else \"N/A\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting contact info: {e}\")\n",
    "\n",
    "    return contact_info\n",
    "\n",
    "def save_to_csv(data, filename='company_data.csv'):\n",
    "    keys = data[0].keys()\n",
    "    with open(filename, 'w', newline='') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data)\n",
    "\n",
    "def main():\n",
    "    company_name = input(\"Enter the company name: \")\n",
    "    website_url = search_google(company_name)\n",
    "    \n",
    "    if website_url:\n",
    "        contact_info = extract_contact_info(website_url)\n",
    "        contact_info['website_url'] = website_url\n",
    "        save_to_csv([contact_info])\n",
    "        print(\"Data saved to CSV.\")\n",
    "    else:\n",
    "        print(\"Failed to find the company's website.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
